# DATA55100 - Unsupervised Machine Learning
Lewis University Spring 2, 2021

<br />

### Week 2 Assignment - Dimensionality Reduction: 
#### DATA55100_PCA.m; DATA55100_LDA.m
Principal Components Analysis (PCA) and Linear Discriminate Analysis (LDA) are both linear transformation techniques used for dimensionality reduction in order to transform the data into a new space which maximizes the variance (PCA) or the class separation (LDA). PCA in particular, is an unsupervised linear transformation method which is used to reduce the dimension number from high dimensional data into lower dimensions by selecting the ‘principal components’ which are used as the new eigenvector axes in the direction of the most variance to maximize the variance or information contained within the data. LDA on the other hand, is a supervised linear transformation method which uses class labels and a mean eigenvector instead of variance to select new axes for reducing the dimensionality based on maximizing class separation. Generally speaking, the first principal component – variance or mean vector, carries the most information and this trend continues down the axes numbers, meaning that when we discard principal components to decrease the dimensions we start with those that contain the least amount of information. PCA looks for the eigenvectors which best describe the information within the data. LDA on the other hand discriminates between classes and the data represents maximizing the between-class measure and minimizing the within-class measure.

<br />

### Week 3 Assignment - Clustering: 
#### MGolf_DATA55000_A2.ipynb 
Perceptron, I

<br />

### Week 4 Assignment - Dimensionality Reduction: 
#### DATA55100_PCA.m; DATA55100_LDA.m
Principal Components Analysis (PCA)

<br />
